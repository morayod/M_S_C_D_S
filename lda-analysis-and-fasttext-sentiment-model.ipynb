{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"!pip install demoji","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:20:56.299856Z","iopub.execute_input":"2023-09-18T15:20:56.300527Z","iopub.status.idle":"2023-09-18T15:21:11.905097Z","shell.execute_reply.started":"2023-09-18T15:20:56.300476Z","shell.execute_reply":"2023-09-18T15:21:11.903686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport torch\nimport numpy as np\nimport sqlite3\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nimport unicodedata as uni\nimport demoji\nfrom tqdm import tqdm\nimport spacy\nimport requests\nimport gensim.corpora as corpora\nfrom gensim.models import Phrases\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:21:11.908180Z","iopub.execute_input":"2023-09-18T15:21:11.909193Z","iopub.status.idle":"2023-09-18T15:21:11.919739Z","shell.execute_reply.started":"2023-09-18T15:21:11.909149Z","shell.execute_reply":"2023-09-18T15:21:11.918262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import file","metadata":{}},{"cell_type":"code","source":"url = 'https://raw.githubusercontent.com/morayod/M_S_C_D_S/main/reviews.csv'\ndf = pd.read_csv(url, encoding=\"utf-8\")\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:21:11.922080Z","iopub.execute_input":"2023-09-18T15:21:11.922556Z","iopub.status.idle":"2023-09-18T15:21:12.461935Z","shell.execute_reply.started":"2023-09-18T15:21:11.922520Z","shell.execute_reply":"2023-09-18T15:21:12.460543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Understand Data","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:21:12.464700Z","iopub.execute_input":"2023-09-18T15:21:12.465040Z","iopub.status.idle":"2023-09-18T15:21:12.502173Z","shell.execute_reply.started":"2023-09-18T15:21:12.465010Z","shell.execute_reply":"2023-09-18T15:21:12.500218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove URLS","metadata":{"execution":{"iopub.status.busy":"2023-09-05T19:53:03.388515Z","iopub.execute_input":"2023-09-05T19:53:03.388966Z","iopub.status.idle":"2023-09-05T19:53:03.401410Z","shell.execute_reply.started":"2023-09-05T19:53:03.388926Z","shell.execute_reply":"2023-09-05T19:53:03.394948Z"}}},{"cell_type":"code","source":"import re\n\n\ndef remove_url(text):\n    text = re.sub(r\"http\\S+\", \"\", text)\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:21:12.504295Z","iopub.execute_input":"2023-09-18T15:21:12.504701Z","iopub.status.idle":"2023-09-18T15:21:12.513522Z","shell.execute_reply.started":"2023-09-18T15:21:12.504668Z","shell.execute_reply":"2023-09-18T15:21:12.511303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clean emojis","metadata":{"execution":{"iopub.status.busy":"2023-09-05T19:58:32.658675Z","iopub.status.idle":"2023-09-05T19:58:32.659506Z","shell.execute_reply.started":"2023-09-05T19:58:32.659225Z","shell.execute_reply":"2023-09-05T19:58:32.659253Z"}}},{"cell_type":"code","source":"def handle_emoji(string):\n    emojis = demoji.findall(string)\n\n    for emoji in emojis:\n        string = string.replace(emoji, \" \" + emojis[emoji].split(\":\")[0])\n\n    return string\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:21:12.515701Z","iopub.execute_input":"2023-09-18T15:21:12.516208Z","iopub.status.idle":"2023-09-18T15:21:12.525780Z","shell.execute_reply.started":"2023-09-18T15:21:12.516171Z","shell.execute_reply":"2023-09-18T15:21:12.524183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenise words","metadata":{}},{"cell_type":"code","source":"def word_tokeniser(text):\n    text = text.lower()\n    text = text.split()\n\n    return text\n\n\nsample = \"Hi Everyone I really love the playlists on this app.\"\nprint(sample)\nprint(word_tokeniser(sample))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:21:12.527527Z","iopub.execute_input":"2023-09-18T15:21:12.527949Z","iopub.status.idle":"2023-09-18T15:21:12.543414Z","shell.execute_reply.started":"2023-09-18T15:21:12.527917Z","shell.execute_reply":"2023-09-18T15:21:12.541986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Here I created a custom list of stopwords including sentiment words, and the name of the app as well as other unimportant words","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n#Adding in extra stopwords related to sentiment and unneeded characters\ncustom_stopwords = [\"spotify\", \"i\", \"I\", \"app\", \"happy\", \"sad\", \"work\", \"hi\", \"fun\", \"idk\", \"lol\", \"ive\", \"trash\", \"god\", \"guys\", \"dont\", \"worst\", \"days\", \"right\", \"thanks\", \"lots\", \"week\", \"nice\", \"kind\", \"day\", \"ca\", \"garbage\", \"today\", \"sucks\", \"awesome\", \"everytime\", \"bit\", \"thank\", \"easy\", \"things\", \"keeps\", \"people\", \"lot\", \"thing\", \"way\", \"times\", \"im\", \"angry\", \"ve\", \"music\", \"great\", \"don\", \"want\", \"good\", \"really\", \"love\", \"hate\", \"songs\", \"song\", \"like\", \"just\"]\nen_stopwords = set(stopwords.words('english'))\nall_stopwords = set(custom_stopwords).union(set(ENGLISH_STOP_WORDS).union(en_stopwords))\nprint(f\"Stop Words in English : \\n{ all_stopwords}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:21:12.545103Z","iopub.execute_input":"2023-09-18T15:21:12.545487Z","iopub.status.idle":"2023-09-18T15:21:12.564411Z","shell.execute_reply.started":"2023-09-18T15:21:12.545457Z","shell.execute_reply":"2023-09-18T15:21:12.563177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove Stopwords function","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(text):\n    text = [word for word in text if word not in all_stopwords]\n    return text\n\n\nprint(f\"Before removing stopwords : {word_tokeniser(sample)}\")\nprint(f\"After removing stopwords : {remove_stopwords(word_tokeniser(sample))}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:21:12.565971Z","iopub.execute_input":"2023-09-18T15:21:12.566462Z","iopub.status.idle":"2023-09-18T15:21:12.584178Z","shell.execute_reply.started":"2023-09-18T15:21:12.566399Z","shell.execute_reply":"2023-09-18T15:21:12.583034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lemmatisation step below","metadata":{}},{"cell_type":"code","source":"sp = spacy.load(\"en_core_web_sm\")\ndef lemmatisation(text):\n\n    text = \" \".join(text)\n    token = sp(text)\n\n    text = [word.lemma_ for word in token]\n    return text\n\n\nprint(f\"Before Lemmatisation : {word_tokeniser(sample)}\")\nprint(f\"After Lemmatisation : {lemmatisation(word_tokeniser(sample))}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:21:12.590851Z","iopub.execute_input":"2023-09-18T15:21:12.591245Z","iopub.status.idle":"2023-09-18T15:21:13.914604Z","shell.execute_reply.started":"2023-09-18T15:21:12.591215Z","shell.execute_reply":"2023-09-18T15:21:13.913319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define pre-processing function","metadata":{}},{"cell_type":"code","source":"def preprocessing(text):\n    \n    text = remove_url(text) \n    text = uni.normalize('NFKD', text)\n    text = handle_emoji(text)\n    text = text.lower() \n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = word_tokeniser(text)\n    text = lemmatisation(text)\n    text = remove_stopwords(text)\n\n    text = \" \".join(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:23:06.969223Z","iopub.execute_input":"2023-09-18T15:23:06.970130Z","iopub.status.idle":"2023-09-18T15:23:06.978680Z","shell.execute_reply.started":"2023-09-18T15:23:06.970089Z","shell.execute_reply":"2023-09-18T15:23:06.976333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply pre-processing","metadata":{}},{"cell_type":"code","source":"tqdm.pandas()\n\ndf['clean_review'] = df['Review'].progress_apply(lambda x: preprocessing(x))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:23:06.993811Z","iopub.execute_input":"2023-09-18T15:23:06.994405Z","iopub.status.idle":"2023-09-18T15:37:22.779261Z","shell.execute_reply.started":"2023-09-18T15:23:06.994365Z","shell.execute_reply":"2023-09-18T15:37:22.777869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### See what cleaned review looks like","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:37:22.781493Z","iopub.execute_input":"2023-09-18T15:37:22.782696Z","iopub.status.idle":"2023-09-18T15:37:22.798660Z","shell.execute_reply.started":"2023-09-18T15:37:22.782643Z","shell.execute_reply":"2023-09-18T15:37:22.797376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenise clean review","metadata":{}},{"cell_type":"code","source":"tqdm.pandas()\n\ndf['tokenised_review'] = df['clean_review'].progress_map(word_tokeniser)\ndata_words = df['tokenised_review'].values.tolist()\n\ndf.head(10)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:37:22.799989Z","iopub.execute_input":"2023-09-18T15:37:22.800350Z","iopub.status.idle":"2023-09-18T15:37:23.101478Z","shell.execute_reply.started":"2023-09-18T15:37:22.800318Z","shell.execute_reply":"2023-09-18T15:37:23.100025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create list of all words","metadata":{}},{"cell_type":"code","source":"data_words = df['tokenised_review'].values.tolist()\nlen(data_words)\nall_words = df['Review'].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:37:23.104181Z","iopub.execute_input":"2023-09-18T15:37:23.104598Z","iopub.status.idle":"2023-09-18T15:37:23.116851Z","shell.execute_reply.started":"2023-09-18T15:37:23.104565Z","shell.execute_reply":"2023-09-18T15:37:23.114826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a dictionary and corpus","metadata":{"execution":{"iopub.status.busy":"2023-09-05T20:17:50.788437Z","iopub.execute_input":"2023-09-05T20:17:50.788880Z","iopub.status.idle":"2023-09-05T20:17:50.797226Z","shell.execute_reply.started":"2023-09-05T20:17:50.788846Z","shell.execute_reply":"2023-09-05T20:17:50.795304Z"}}},{"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_words)\n# Create Corpus\ntexts = data_words\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\ncorpus_bigram = [id2word.doc2bow(doc) for doc in data_words]\n# View\nprint(corpus[:1][0][:30])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:37:23.117952Z","iopub.execute_input":"2023-09-18T15:37:23.118357Z","iopub.status.idle":"2023-09-18T15:37:27.034468Z","shell.execute_reply.started":"2023-09-18T15:37:23.118323Z","shell.execute_reply":"2023-09-18T15:37:27.033296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Example preprocessing ","metadata":{}},{"cell_type":"code","source":"tokenised_corpus = [[id2word[word_id] for (word_id, count) in doc] for doc in corpus]\n\nprocessed_text = preprocessing(\"I love spotify so much it's the best thing every and I'm impressed by the audio quality\")\nprint(processed_text)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:37:27.036662Z","iopub.execute_input":"2023-09-18T15:37:27.037651Z","iopub.status.idle":"2023-09-18T15:37:27.515241Z","shell.execute_reply.started":"2023-09-18T15:37:27.037605Z","shell.execute_reply":"2023-09-18T15:37:27.513979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LDA model building for topic modelling","metadata":{}},{"cell_type":"code","source":"from gensim.models import LdaMulticore\nfrom gensim.models import LdaModel\nfrom pprint import pprint\n\n# number of topics\nnum_topics = 10 \n# Build LDA model\nlda_model = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics, iterations=400)\n\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:37:27.516529Z","iopub.execute_input":"2023-09-18T15:37:27.516861Z","iopub.status.idle":"2023-09-18T15:37:52.842881Z","shell.execute_reply.started":"2023-09-18T15:37:27.516833Z","shell.execute_reply":"2023-09-18T15:37:52.841357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although topics aren't really well defined, I've manually chosen these labels from the topics for the further analysis:\n\nadvert,\nsubscription_or_cost,\nsong_selection,\nuser_experience,\ncomparison,\naudio_quality,\nsupport,\npodcasts,\nconnectivity,\npodcast,\nrecommendation,\nplaylists\n\nBelow I will visualise the clusters from the LDA analysis","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nfrom bokeh.io import push_notebook, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import Label\nfrom bokeh.palettes import Category10\noutput_notebook()\n\n# Get Document-Topic Distributions\ndoc_topic_dists = np.zeros((len(corpus), num_topics))\nfor doc_num, doc_topics in enumerate(lda_model[corpus_bigram]):\n    for topic, prob in doc_topics:\n        doc_topic_dists[doc_num, topic] = prob\nprint(doc_topic_dists.shape)\n\n\n# Use t-SNE to reduce dimensionality\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(doc_topic_dists)\n\n# Visualising using Bokeh\nn_topics = lda_model.num_topics\ncolormap = np.array(Category10[10])\n_lda_keys = np.array(doc_topic_dists).argmax(axis=1).tolist()\n\n# Finding the centroid of each topic\n_mean_topic_vectors = []\nfor t in range(n_topics):\n    if colormap[_lda_keys].tolist().count(colormap[t]) > 0:\n        _mean_topic_vectors.append(tsne_lda[colormap[_lda_keys] == colormap[t]].mean(axis=0))\n        \ntop_3_words_lda = []\nfor t in range(n_topics):\n    topic_words = lda_model.show_topic(t, 3)\n    words_for_topic = [word for word, score in topic_words]\n    top_3_words_lda.append(\", \".join(words_for_topic))\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:37:52.844866Z","iopub.execute_input":"2023-09-18T15:37:52.845227Z","iopub.status.idle":"2023-09-18T15:45:01.487780Z","shell.execute_reply.started":"2023-09-18T15:37:52.845191Z","shell.execute_reply":"2023-09-18T15:45:01.486197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean sigma is not great but this is fine for our type of analysis","metadata":{}},{"cell_type":"code","source":"plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), width=700, height=700)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=colormap[_lda_keys])\nfor t in range(n_topics):\n    label = Label(x=_mean_topic_vectors[t][0], y=_mean_topic_vectors[t][1], \n                  text=top_3_words_lda[t], text_color=colormap[t])\n    plot.add_layout(label)\n\nshow(plot, notebook_handle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:45:01.489620Z","iopub.execute_input":"2023-09-18T15:45:01.490523Z","iopub.status.idle":"2023-09-18T15:45:02.576755Z","shell.execute_reply.started":"2023-09-18T15:45:01.490487Z","shell.execute_reply":"2023-09-18T15:45:02.575504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clusters are not well defined but this is fine as we were able to pre-define topics for our analysis below\n\nNext to train the FastText model","metadata":{}},{"cell_type":"code","source":"input_url = 'https://raw.githubusercontent.com/morayod/M_S_C_D_S/main/assistant_label_responses.txt'\n\nresponse = requests.get(input_url)\n\nfile_content = response.text\n\n# Display the first 500 characters\n    \nprint(file_content[:500])  \n    \n# Calculate the word count\nword_count = len(file_content.split())\nprint(f\"Word count: {word_count}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:45:02.582009Z","iopub.execute_input":"2023-09-18T15:45:02.582404Z","iopub.status.idle":"2023-09-18T15:45:02.908539Z","shell.execute_reply.started":"2023-09-18T15:45:02.582374Z","shell.execute_reply":"2023-09-18T15:45:02.907300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lines = response.text.split('\\n')\nlen(lines)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:45:02.909901Z","iopub.execute_input":"2023-09-18T15:45:02.910253Z","iopub.status.idle":"2023-09-18T15:45:02.920764Z","shell.execute_reply.started":"2023-09-18T15:45:02.910226Z","shell.execute_reply":"2023-09-18T15:45:02.919301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split training and validation data and train","metadata":{}},{"cell_type":"code","source":"import fasttext\ntrain_data = \"\\n\".join(lines[:1735])  # Adjust the number of lines you want for training\nvalid_data = \"\\n\".join(lines[1735:])  # Adjust the number of lines for validation\n# Split the data into train and validation sets\nwith open(\"spotify.train\", \"w\", encoding=\"utf-8\") as train_file:\n        train_file.write(train_data)\n        \nwith open(\"spotify.valid\", \"w\", encoding=\"utf-8\") as valid_file:\n        valid_file.write(valid_data)\n\n\nmodel = fasttext.train_supervised(input=\"spotify.train\", lr=0.73, epoch=67, wordNgrams=1, bucket=200000, dim=50, loss='ova', seed=7077)\nmodel.save_model(\"model_spotify_final.bin\") ","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:47:58.113848Z","iopub.execute_input":"2023-09-18T15:47:58.114259Z","iopub.status.idle":"2023-09-18T15:47:58.873936Z","shell.execute_reply.started":"2023-09-18T15:47:58.114229Z","shell.execute_reply":"2023-09-18T15:47:58.872684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test in a sample review not from the training set","metadata":{}},{"cell_type":"code","source":"model.predict(\"I hate the ads despite paying so much, and audio just sounds tinny\", k=12)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:47:58.875791Z","iopub.execute_input":"2023-09-18T15:47:58.876613Z","iopub.status.idle":"2023-09-18T15:47:58.885230Z","shell.execute_reply.started":"2023-09-18T15:47:58.876579Z","shell.execute_reply":"2023-09-18T15:47:58.884180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model correctly has high probability for the above example for the aspects \"advert\", audio_quality\" and \"subscription_or_cost\" ","metadata":{}},{"cell_type":"markdown","source":"## Calculate the optimal precision and recall picking the best threshold","metadata":{}},{"cell_type":"code","source":"thresholds = np.linspace(1, 0, 100)  \nprecisions = []\nrecalls = []\nf1_scores = []\nnum_labels = 5\n\nfor threshold in thresholds:\n    _, precision, recall = model.test(\"spotify.valid\", k=num_labels, threshold=threshold)\n    \n    precisions.append(precision)\n    recalls.append(recall)\n    f1_scores.append(2 * (precision * recall) / (precision + recall))\n\n# Plotting\nplt.figure(figsize=(10,7))\nplt.plot(recalls, precisions, color='blue')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.grid()\n\n# Highlight the maximum F1-score point\noptimal_idx = np.argmax(f1_scores)\noptimal_threshold = thresholds[optimal_idx]\nplt.scatter(recalls[optimal_idx], precisions[optimal_idx], color='red')\nplt.annotate(f'Threshold:{optimal_threshold:.2f}', (recalls[optimal_idx], precisions[optimal_idx]))\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:47:58.886618Z","iopub.execute_input":"2023-09-18T15:47:58.886982Z","iopub.status.idle":"2023-09-18T15:48:00.240719Z","shell.execute_reply.started":"2023-09-18T15:47:58.886954Z","shell.execute_reply":"2023-09-18T15:48:00.239120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate precision and recall using optimal threshold. ","metadata":{}},{"cell_type":"code","source":"threshold = optimal_threshold\nmodel.test(\"spotify.valid\", k=num_labels, threshold=threshold)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:48:00.243844Z","iopub.execute_input":"2023-09-18T15:48:00.244211Z","iopub.status.idle":"2023-09-18T15:48:00.256403Z","shell.execute_reply.started":"2023-09-18T15:48:00.244175Z","shell.execute_reply":"2023-09-18T15:48:00.255176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's not a bad precision or recall - we can adjust the threshold hgiher for increased precision but there will be a tradeoff for recall\nDefine an aspect extraction function","metadata":{}},{"cell_type":"code","source":"def get_aspects(text):\n    try:\n        return model.predict(text, k=num_labels, threshold=threshold)\n    except:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:48:00.258288Z","iopub.execute_input":"2023-09-18T15:48:00.259677Z","iopub.status.idle":"2023-09-18T15:48:00.268281Z","shell.execute_reply.started":"2023-09-18T15:48:00.259560Z","shell.execute_reply":"2023-09-18T15:48:00.266678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas(desc=\"Calculating Similarities\")\ndf['label'] = df['Review'].progress_apply(lambda text: get_aspects(text))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:48:00.270728Z","iopub.execute_input":"2023-09-18T15:48:00.271558Z","iopub.status.idle":"2023-09-18T15:48:01.468826Z","shell.execute_reply.started":"2023-09-18T15:48:00.271512Z","shell.execute_reply":"2023-09-18T15:48:01.467669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 100)\ndf.tail(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:48:01.470413Z","iopub.execute_input":"2023-09-18T15:48:01.470886Z","iopub.status.idle":"2023-09-18T15:48:01.497198Z","shell.execute_reply.started":"2023-09-18T15:48:01.470844Z","shell.execute_reply":"2023-09-18T15:48:01.495725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nfrom transformers import pipeline\n\nsentiment_pipeline = pipeline('sentiment-analysis')\n\ndef split_on_conjunctions(text):\n    splits = re.split(r'\\b(but|or|because|so|although|though|while)\\b|\\s*,\\s*', text)\n    return [s.strip() for s in splits if s]\n\ndef get_sentiment(text):\n    result = sentiment_pipeline(text)\n    return result[0]['label']\n\ndef get_absa(review):\n    sentences = sent_tokenize(review)\n    label_sentiments = {}\n\n    for sentence in sentences:\n        segments = split_on_conjunctions(sentence)\n        for segment in segments:\n            sentiment = get_sentiment(segment)\n            if sentiment == 'POSITIVE':\n                sentiment_label = \"Positive\"\n            elif sentiment == 'NEGATIVE':\n                sentiment_label = \"Negative\"\n            else:\n                sentiment_label = \"Neutral\"\n\n            labels, probabilities = model.predict(segment, threshold=threshold, k=num_labels)\n\n            for i, label in enumerate(labels):\n                stripped_label = label.replace(\"__label__\", \"\")\n                if stripped_label not in label_sentiments or probabilities[i] > label_sentiments[stripped_label][1]:\n                    label_sentiments[stripped_label] = (sentiment_label, probabilities[i])\n\n    # Remtving the probabilities and keeping only the sentiments for final output\n    final_sentiments = {label: sentiment[0] for label, sentiment in label_sentiments.items()}\n    return final_sentiments\n    \n\nreview = (\"Easiest and most convenient way for a student to stream music,\" \n\" but the app is plagued with issues, like pausing randomly on my Samsung\" \n\" S8 *edit* Issues still persist on Samsung S21. I'm paying far too much for the \"\n\"premium I wish it cost less but I kind of enjoy the listening to the podcasts \"\n\"available. Please help  \"\n\"the customer support team is lacking as they don't communicate nor reply and I absolutely hate how slow it is to connect my device on wifi and all the buzzing sounds are annoying\")\n\nget_absa(review)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:48:01.499110Z","iopub.execute_input":"2023-09-18T15:48:01.499686Z","iopub.status.idle":"2023-09-18T15:48:02.752961Z","shell.execute_reply.started":"2023-09-18T15:48:01.499651Z","shell.execute_reply":"2023-09-18T15:48:02.751865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datafile = 'https://raw.githubusercontent.com/morayod/M_S_C_D_S/main/randomly%20shuffled_reviews_only.csv'","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:48:02.754489Z","iopub.execute_input":"2023-09-18T15:48:02.754813Z","iopub.status.idle":"2023-09-18T15:48:02.759383Z","shell.execute_reply.started":"2023-09-18T15:48:02.754786Z","shell.execute_reply":"2023-09-18T15:48:02.758462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(datafile, encoding=\"utf-8\")\ntest_df_100 = test_df.head(100)\ntqdm.pandas(desc=\"Calculating Similarities\")\ntest_df_100['absa'] = test_df_100['Review'].progress_apply(lambda text: get_absa(text))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:48:02.762595Z","iopub.execute_input":"2023-09-18T15:48:02.763310Z","iopub.status.idle":"2023-09-18T15:48:16.005870Z","shell.execute_reply.started":"2023-09-18T15:48:02.763270Z","shell.execute_reply":"2023-09-18T15:48:16.004615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_100","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:48:16.008064Z","iopub.execute_input":"2023-09-18T15:48:16.010816Z","iopub.status.idle":"2023-09-18T15:48:16.077894Z","shell.execute_reply.started":"2023-09-18T15:48:16.010768Z","shell.execute_reply":"2023-09-18T15:48:16.076365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A great job on my sample reviews!","metadata":{}},{"cell_type":"code","source":"test_df_100.to_csv(\"lda_model_output.csv\", encoding='utf-8', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:48:16.080002Z","iopub.execute_input":"2023-09-18T15:48:16.080379Z","iopub.status.idle":"2023-09-18T15:48:16.088883Z","shell.execute_reply.started":"2023-09-18T15:48:16.080345Z","shell.execute_reply":"2023-09-18T15:48:16.087487Z"},"trusted":true},"execution_count":null,"outputs":[]}]}