{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-15T23:06:01.688078Z","iopub.execute_input":"2023-09-15T23:06:01.688550Z","iopub.status.idle":"2023-09-15T23:06:01.750923Z","shell.execute_reply.started":"2023-09-15T23:06:01.688516Z","shell.execute_reply":"2023-09-15T23:06:01.749636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\nimport sqlite3\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport warnings","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:01.753047Z","iopub.execute_input":"2023-09-15T23:06:01.753433Z","iopub.status.idle":"2023-09-15T23:06:06.490262Z","shell.execute_reply.started":"2023-09-15T23:06:01.753401Z","shell.execute_reply":"2023-09-15T23:06:06.489181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import file","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndatafile = '/kaggle/input/spotify-app-reviews-2022/reviews.csv'\ndf = pd.read_csv(datafile, encoding=\"utf-8\")\n\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:06.491432Z","iopub.execute_input":"2023-09-15T23:06:06.491887Z","iopub.status.idle":"2023-09-15T23:06:06.859258Z","shell.execute_reply.started":"2023-09-15T23:06:06.491857Z","shell.execute_reply":"2023-09-15T23:06:06.857949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Understand Data","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:06.862244Z","iopub.execute_input":"2023-09-15T23:06:06.862582Z","iopub.status.idle":"2023-09-15T23:06:06.902663Z","shell.execute_reply.started":"2023-09-15T23:06:06.862555Z","shell.execute_reply":"2023-09-15T23:06:06.901547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove URLS","metadata":{"execution":{"iopub.status.busy":"2023-09-05T19:53:03.388515Z","iopub.execute_input":"2023-09-05T19:53:03.388966Z","iopub.status.idle":"2023-09-05T19:53:03.401410Z","shell.execute_reply.started":"2023-09-05T19:53:03.388926Z","shell.execute_reply":"2023-09-05T19:53:03.394948Z"}}},{"cell_type":"code","source":"import re\n\n\ndef remove_url(text):\n    text = re.sub(r\"http\\S+\", \"\", text)\n    return text\n\n\n# Example string with weird font characters plus an URL which we gonna remove.\nsample = \"‚Ñçùïö ùîºùïßùïñùï£ùï™ùï†ùïüùïñ \\n https://www.kaggle.com/ üòä\"\nprint(f\"Text before removing url:- \\n {sample}\")\n\nsample = remove_url(sample)\nprint(f\"Text after removing url:- \\n {sample}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:06.904002Z","iopub.execute_input":"2023-09-15T23:06:06.904560Z","iopub.status.idle":"2023-09-15T23:06:06.911613Z","shell.execute_reply.started":"2023-09-15T23:06:06.904511Z","shell.execute_reply":"2023-09-15T23:06:06.910358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalise unicode data","metadata":{}},{"cell_type":"code","source":"import unicodedata as uni\n\nprint(f\"Text before Unicode Normalization:- \\n {sample}\")\n\nsample = uni.normalize('NFKD', sample)\nprint(f\"Text after Unicode Normalization:- \\n {sample}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:06.913362Z","iopub.execute_input":"2023-09-15T23:06:06.913697Z","iopub.status.idle":"2023-09-15T23:06:06.927109Z","shell.execute_reply.started":"2023-09-15T23:06:06.913664Z","shell.execute_reply":"2023-09-15T23:06:06.925832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Install Demoji library","metadata":{}},{"cell_type":"code","source":"!pip install demoji","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:06.928975Z","iopub.execute_input":"2023-09-15T23:06:06.929408Z","iopub.status.idle":"2023-09-15T23:06:20.222977Z","shell.execute_reply.started":"2023-09-15T23:06:06.929373Z","shell.execute_reply":"2023-09-15T23:06:20.221697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clean emojis","metadata":{"execution":{"iopub.status.busy":"2023-09-05T19:58:32.658675Z","iopub.status.idle":"2023-09-05T19:58:32.659506Z","shell.execute_reply.started":"2023-09-05T19:58:32.659225Z","shell.execute_reply":"2023-09-05T19:58:32.659253Z"}}},{"cell_type":"code","source":"import demoji\n\n\ndef handle_emoji(string):\n    emojis = demoji.findall(string)\n\n    for emoji in emojis:\n        string = string.replace(emoji, \" \" + emojis[emoji].split(\":\")[0])\n\n    return string\n\n\nprint(f\"Before Handling emoji:- \\n {sample}\")\nprint(f\"After Handling emoji:- \\n {handle_emoji(sample)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:20.224603Z","iopub.execute_input":"2023-09-15T23:06:20.224971Z","iopub.status.idle":"2023-09-15T23:06:20.514135Z","shell.execute_reply.started":"2023-09-15T23:06:20.224940Z","shell.execute_reply":"2023-09-15T23:06:20.512856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenise words","metadata":{}},{"cell_type":"code","source":"def word_tokenizer(text):\n    text = text.lower()\n    text = text.split()\n\n    return text\n\n\nsample = \"Hi Everyone I really love the playlists on this app.\"\nprint(sample)\nprint(word_tokenizer(sample))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:20.515346Z","iopub.execute_input":"2023-09-15T23:06:20.515672Z","iopub.status.idle":"2023-09-15T23:06:20.522817Z","shell.execute_reply.started":"2023-09-15T23:06:20.515645Z","shell.execute_reply":"2023-09-15T23:06:20.521368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here I created a custome list of stopwords including sentiment words, and the name of the app as well as other unimportant words","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n#Adding in extra stopwords related to sentiment and unneeded characters\ncustom_stopwords = [\"spotify\", \"i\", \"I\", \"app\", \"happy\", \"sad\", \"work\", \"hi\", \"fun\", \"idk\", \"lol\", \"ive\", \"trash\", \"god\", \"guys\", \"dont\", \"worst\", \"days\", \"right\", \"thanks\", \"lots\", \"week\", \"nice\", \"kind\", \"day\", \"ca\", \"garbage\", \"today\", \"sucks\", \"awesome\", \"everytime\", \"bit\", \"thank\", \"easy\", \"things\", \"keeps\", \"people\", \"lot\", \"thing\", \"way\", \"times\", \"im\", \"angry\", \"ve\", \"music\", \"great\", \"don\", \"want\", \"good\", \"really\", \"love\", \"hate\", \"songs\", \"song\", \"like\", \"just\"]\nen_stopwords = set(stopwords.words('english'))\nall_stopwords = set(custom_stopwords).union(set(ENGLISH_STOP_WORDS).union(en_stopwords))\nprint(f\"Stop Words in English : \\n{ all_stopwords}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:20.529786Z","iopub.execute_input":"2023-09-15T23:06:20.530472Z","iopub.status.idle":"2023-09-15T23:06:20.545819Z","shell.execute_reply.started":"2023-09-15T23:06:20.530394Z","shell.execute_reply":"2023-09-15T23:06:20.544368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove Stopwords function","metadata":{}},{"cell_type":"code","source":"\ndef remove_stopwords(text):\n    text = [word for word in text if word not in all_stopwords]\n    return text\n\n\nprint(f\"Before removing stopwords : {word_tokenizer(sample)}\")\nprint(f\"After removing stopwords : {remove_stopwords(word_tokenizer(sample))}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:20.547265Z","iopub.execute_input":"2023-09-15T23:06:20.547651Z","iopub.status.idle":"2023-09-15T23:06:20.554787Z","shell.execute_reply.started":"2023-09-15T23:06:20.547620Z","shell.execute_reply":"2023-09-15T23:06:20.553747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lemmatisation step below","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n\nstemmer = PorterStemmer()\n\n\ndef stemming(text):\n\n    text = [stemmer.stem(word) for word in text]\n    return text\n\n\nsample = \"I am creating a Notebook\"\nprint(f\"Before Stemming : {(sample)}\")\nprint(f\"After Stemming : {stemming(word_tokenizer(sample))}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:20.556354Z","iopub.execute_input":"2023-09-15T23:06:20.556709Z","iopub.status.idle":"2023-09-15T23:06:20.570170Z","shell.execute_reply.started":"2023-09-15T23:06:20.556679Z","shell.execute_reply":"2023-09-15T23:06:20.569208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import Spacy","metadata":{}},{"cell_type":"code","source":"import spacy\n\nsp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:20.571293Z","iopub.execute_input":"2023-09-15T23:06:20.571674Z","iopub.status.idle":"2023-09-15T23:06:32.737418Z","shell.execute_reply.started":"2023-09-15T23:06:20.571642Z","shell.execute_reply":"2023-09-15T23:06:32.735884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alternative lemmatisation using Spacy library","metadata":{}},{"cell_type":"code","source":"def lemmatization(text):\n\n    # text = [sp(word).lemma_ for word in text]\n\n    text = \" \".join(text)\n    token = sp(text)\n\n    text = [word.lemma_ for word in token]\n    return text\n\n\nprint(f\"Before Lemmatization : {word_tokenizer(sample)}\")\nprint(f\"After Lemmatization : {lemmatization(word_tokenizer(sample))}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:32.739196Z","iopub.execute_input":"2023-09-15T23:06:32.739848Z","iopub.status.idle":"2023-09-15T23:06:32.780815Z","shell.execute_reply.started":"2023-09-15T23:06:32.739812Z","shell.execute_reply":"2023-09-15T23:06:32.779767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define pre-processing function","metadata":{}},{"cell_type":"code","source":"from gensim.models import Phrases\n\ndef preprocessing(text):\n    \n    text = remove_url(text) \n    text = uni.normalize('NFKD', text)\n    text = handle_emoji(text)\n    text = text.lower() \n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = word_tokenizer(text)\n    # text = stemming(text)\n    text = lemmatization(text)\n    text = remove_stopwords(text)\n\n    text = \" \".join(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:06:32.782202Z","iopub.execute_input":"2023-09-15T23:06:32.782540Z","iopub.status.idle":"2023-09-15T23:07:06.043699Z","shell.execute_reply.started":"2023-09-15T23:06:32.782509Z","shell.execute_reply":"2023-09-15T23:07:06.042317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply pre-processing","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ntqdm.pandas()\n\ndf['clean_review'] = df['Review'].progress_apply(lambda x: preprocessing(x))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:07:06.045189Z","iopub.execute_input":"2023-09-15T23:07:06.045561Z","iopub.status.idle":"2023-09-15T23:19:19.500461Z","shell.execute_reply.started":"2023-09-15T23:07:06.045529Z","shell.execute_reply":"2023-09-15T23:19:19.499212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See what cleaned review looks like","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:19:19.502070Z","iopub.execute_input":"2023-09-15T23:19:19.502536Z","iopub.status.idle":"2023-09-15T23:19:19.518452Z","shell.execute_reply.started":"2023-09-15T23:19:19.502494Z","shell.execute_reply":"2023-09-15T23:19:19.517175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenise clean review","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\ntqdm.pandas()\n\ndf['clean_review2'] = df['clean_review'].progress_map(word_tokenizer)\ndata_words = df['clean_review2'].values.tolist()\n\ndf.head(20)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:19:19.520256Z","iopub.execute_input":"2023-09-15T23:19:19.520596Z","iopub.status.idle":"2023-09-15T23:19:19.750252Z","shell.execute_reply.started":"2023-09-15T23:19:19.520567Z","shell.execute_reply":"2023-09-15T23:19:19.749004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create list of all words","metadata":{}},{"cell_type":"code","source":"data_words = df['clean_review2'].values.tolist()\nlen(data_words)\nall_words = df['Review'].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:19:19.751775Z","iopub.execute_input":"2023-09-15T23:19:19.752195Z","iopub.status.idle":"2023-09-15T23:19:19.763144Z","shell.execute_reply.started":"2023-09-15T23:19:19.752161Z","shell.execute_reply":"2023-09-15T23:19:19.761933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a disctionary and corpus","metadata":{"execution":{"iopub.status.busy":"2023-09-05T20:17:50.788437Z","iopub.execute_input":"2023-09-05T20:17:50.788880Z","iopub.status.idle":"2023-09-05T20:17:50.797226Z","shell.execute_reply.started":"2023-09-05T20:17:50.788846Z","shell.execute_reply":"2023-09-05T20:17:50.795304Z"}}},{"cell_type":"code","source":"import gensim.corpora as corpora\n\n# Create Dictionary\nid2word = corpora.Dictionary(data_words)\n# Create Corpus\ntexts = data_words\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\ncorpus_bigram = [id2word.doc2bow(doc) for doc in data_words]\n# View\nprint(corpus[:1][0][:30])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:19:19.764626Z","iopub.execute_input":"2023-09-15T23:19:19.764960Z","iopub.status.idle":"2023-09-15T23:19:22.542911Z","shell.execute_reply.started":"2023-09-15T23:19:19.764930Z","shell.execute_reply":"2023-09-15T23:19:22.541731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Example preprocessing ","metadata":{}},{"cell_type":"code","source":"\ntokenized_corpus = [[id2word[word_id] for (word_id, count) in doc] for doc in corpus]\n\nprocessed_text = preprocessing(\"I love spotify so much it's the best thing every and I'm impressed by the audio quality\")\nprint(processed_text)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:19:22.544579Z","iopub.execute_input":"2023-09-15T23:19:22.544955Z","iopub.status.idle":"2023-09-15T23:19:22.910204Z","shell.execute_reply.started":"2023-09-15T23:19:22.544920Z","shell.execute_reply":"2023-09-15T23:19:22.909006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LDA model building for topic modelling","metadata":{}},{"cell_type":"code","source":"from gensim.models import LdaMulticore\nfrom gensim.models import LdaModel\nfrom pprint import pprint\n\n# number of topics\nnum_topics = 10\n# Build LDA model\nlda_model = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics, iterations=400)\n\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:19:22.911951Z","iopub.execute_input":"2023-09-15T23:19:22.912416Z","iopub.status.idle":"2023-09-15T23:19:41.084329Z","shell.execute_reply.started":"2023-09-15T23:19:22.912373Z","shell.execute_reply":"2023-09-15T23:19:41.082947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although topics aren't really well defined, I've manually chosen these labels from the topics for the further analysis:\n\nadvert\nsubscription_or_cost\nsong_selection\nuser_experience\ncomparison\naudio_quality\nsupport\npodcasts\nconnectivity\npodcast\nrecommendation\nplaylists\n\nBelow I will visualise the clusters from the LDA analysis","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.manifold import TSNE\nfrom bokeh.io import push_notebook, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import Label\nfrom bokeh.palettes import Category10\noutput_notebook()\n\n# Get Document-Topic Distributions\ndoc_topic_dists = np.zeros((len(corpus), num_topics))\nfor doc_num, doc_topics in enumerate(lda_model[corpus_bigram]):\n    for topic, prob in doc_topics:\n        doc_topic_dists[doc_num, topic] = prob\nprint(doc_topic_dists.shape)\n\n\n# Use t-SNE to reduce dimensionality\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(doc_topic_dists)\n\n# Visualising using Bokeh\nn_topics = lda_model.num_topics\ncolormap = np.array(Category10[10])\n_lda_keys = np.array(doc_topic_dists).argmax(axis=1).tolist()\n\n# Finding the centroid of each topic\n_mean_topic_vectors = []\nfor t in range(n_topics):\n    if colormap[_lda_keys].tolist().count(colormap[t]) > 0:\n        _mean_topic_vectors.append(tsne_lda[colormap[_lda_keys] == colormap[t]].mean(axis=0))\n        \ntop_3_words_lda = []\nfor t in range(n_topics):\n    topic_words = lda_model.show_topic(t, 3)\n    words_for_topic = [word for word, score in topic_words]\n    top_3_words_lda.append(\", \".join(words_for_topic))\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:19:41.086177Z","iopub.execute_input":"2023-09-15T23:19:41.086567Z","iopub.status.idle":"2023-09-15T23:26:07.941612Z","shell.execute_reply.started":"2023-09-15T23:19:41.086531Z","shell.execute_reply":"2023-09-15T23:26:07.940350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean sigma is not great but this is fine for our type of analysis","metadata":{}},{"cell_type":"code","source":"plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), width=700, height=700)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=colormap[_lda_keys])\nfor t in range(n_topics):\n    label = Label(x=_mean_topic_vectors[t][0], y=_mean_topic_vectors[t][1], \n                  text=top_3_words_lda[t], text_color=colormap[t])\n    plot.add_layout(label)\n\nshow(plot, notebook_handle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:26:07.943566Z","iopub.execute_input":"2023-09-15T23:26:07.944289Z","iopub.status.idle":"2023-09-15T23:26:08.696152Z","shell.execute_reply.started":"2023-09-15T23:26:07.944235Z","shell.execute_reply":"2023-09-15T23:26:08.695209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clusters are not well defined but this is fine as we were able to pre-define topics for our analysis below\n\nNext to train the FastText model","metadata":{}},{"cell_type":"code","source":"# load in open-ai labelled data\ninput_path = '/kaggle/input/final-dataset-msc-3/assistant_label_responses.txt'\nf = open(input_path,'r')\n\n!head /kaggle/input/final-dataset-msc-3/assistant_label_responses.txt\n!wc /kaggle/input/final-dataset-msc-3/assistant_label_responses.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:26:08.698707Z","iopub.execute_input":"2023-09-15T23:26:08.699455Z","iopub.status.idle":"2023-09-15T23:26:10.810719Z","shell.execute_reply.started":"2023-09-15T23:26:08.699415Z","shell.execute_reply":"2023-09-15T23:26:10.808959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocess training data","metadata":{}},{"cell_type":"code","source":"import re\n\ndef preprocess_text(text):\n    # Add spaces around punctuation\n    text = re.sub(r'([.\\!?,\"/()])', r' \\1 ', text)\n    \n    # Convert uppercase to lowercase\n    text = text.lower()\n    \n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n\nwith open(\"/kaggle/input/final-dataset-msc-3/assistant_label_responses.txt\", \"r\", encoding=\"utf-8\") as infile:\n    with open(\"spotify_final_msc-3.preprocessed.txt\", \"w\", encoding=\"utf-8\") as outfile:\n        for line in infile:\n            outfile.write(preprocess_text(line) + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-15T23:26:10.812899Z","iopub.execute_input":"2023-09-15T23:26:10.813437Z","iopub.status.idle":"2023-09-15T23:26:10.887399Z","shell.execute_reply.started":"2023-09-15T23:26:10.813387Z","shell.execute_reply":"2023-09-15T23:26:10.886174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split training and validation data","metadata":{}},{"cell_type":"code","source":"import fasttext\n!tail -n 1735 /kaggle/input/final-dataset-msc-3/assistant_label_responses.txt > spotify.train\n!head -n 434 /kaggle/input/final-dataset-msc-3/assistant_label_responses.txt > spotify.valid\nmodel = fasttext.train_supervised(input=\"spotify.train\", lr=0.73, epoch=67, wordNgrams=1, bucket=200000, dim=50, loss='ova')\nmodel.save_model(\"model_spotify_final.bin\") ","metadata":{"execution":{"iopub.status.busy":"2023-09-16T00:08:09.622819Z","iopub.execute_input":"2023-09-16T00:08:09.623222Z","iopub.status.idle":"2023-09-16T00:08:12.546490Z","shell.execute_reply.started":"2023-09-16T00:08:09.623192Z","shell.execute_reply":"2023-09-16T00:08:12.545376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test in a sample review not from the training set","metadata":{}},{"cell_type":"code","source":"model.predict(\"I hate the adds despite paying so much, and audio just sounds tinny\", k=12)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T00:08:12.548578Z","iopub.execute_input":"2023-09-16T00:08:12.549609Z","iopub.status.idle":"2023-09-16T00:08:12.557937Z","shell.execute_reply.started":"2023-09-16T00:08:12.549570Z","shell.execute_reply":"2023-09-16T00:08:12.556888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model correctly has high probability for the above example for the aspects \"adverts\" and \"subscription_or_cost\" and a borderline acceptable weight for \"user_experience\". However, even though audio quality has the 4th most likely aspect(along with support), the probability is low","metadata":{}},{"cell_type":"markdown","source":"Calculate the optimel precision and recall picking the best threshold","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nthresholds = np.linspace(1, 0, 100)  \nprecisions = []\nrecalls = []\nf1_scores = []\nnum_labels = 5\n\nfor threshold in thresholds:\n    _, precision, recall = model.test(\"spotify.valid\", k=num_labels, threshold=threshold)\n    \n    precisions.append(precision)\n    recalls.append(recall)\n    f1_scores.append(2 * (precision * recall) / (precision + recall))\n\n# Plotting\nplt.figure(figsize=(10,7))\nplt.plot(recalls, precisions, color='blue')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.grid()\n\n# Highlight the maximum F1-score point\noptimal_idx = np.argmax(f1_scores)\noptimal_threshold = thresholds[optimal_idx]\nplt.scatter(recalls[optimal_idx], precisions[optimal_idx], color='red')\nplt.annotate(f'Threshold:{optimal_threshold:.2f}', (recalls[optimal_idx], precisions[optimal_idx]))\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-16T00:08:12.559594Z","iopub.execute_input":"2023-09-16T00:08:12.560609Z","iopub.status.idle":"2023-09-16T00:08:13.184545Z","shell.execute_reply.started":"2023-09-16T00:08:12.560566Z","shell.execute_reply":"2023-09-16T00:08:13.183429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate precision and recall using optimal threshold. ","metadata":{}},{"cell_type":"code","source":"threshold = optimal_threshold\nmodel.test(\"spotify.valid\", k=num_labels, threshold=threshold)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T00:08:13.187070Z","iopub.execute_input":"2023-09-16T00:08:13.188157Z","iopub.status.idle":"2023-09-16T00:08:13.200038Z","shell.execute_reply.started":"2023-09-16T00:08:13.188097Z","shell.execute_reply":"2023-09-16T00:08:13.199081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's not a bad precision or recall - we can adjust the threshold hgiher for increased precision but there will be a tradeoff for recall\nDefine an aspect extraction function","metadata":{}},{"cell_type":"code","source":"def get_aspects(text):\n    try:\n        return model.predict(text, k=num_labels, threshold=threshold)\n    except:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2023-09-16T00:08:13.201528Z","iopub.execute_input":"2023-09-16T00:08:13.201909Z","iopub.status.idle":"2023-09-16T00:08:13.211506Z","shell.execute_reply.started":"2023-09-16T00:08:13.201881Z","shell.execute_reply":"2023-09-16T00:08:13.210407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas(desc=\"Calculating Similarities\")\ndf['label'] = df['Review'].progress_apply(lambda text: get_aspects(text))","metadata":{"execution":{"iopub.status.busy":"2023-09-16T00:08:13.213298Z","iopub.execute_input":"2023-09-16T00:08:13.213754Z","iopub.status.idle":"2023-09-16T00:08:14.232166Z","shell.execute_reply.started":"2023-09-16T00:08:13.213713Z","shell.execute_reply":"2023-09-16T00:08:14.231052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 100)\ndf.tail(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T00:08:14.233712Z","iopub.execute_input":"2023-09-16T00:08:14.234427Z","iopub.status.idle":"2023-09-16T00:08:14.255419Z","shell.execute_reply.started":"2023-09-16T00:08:14.234382Z","shell.execute_reply":"2023-09-16T00:08:14.254529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fasttext\nfrom nltk.tokenize import sent_tokenize\nimport re\nfrom transformers import pipeline\n\nsentiment_pipeline = pipeline('sentiment-analysis')\n\ndef split_on_conjunctions(text):\n    splits = re.split(r'\\b(but|or|because|so|although|though|while)\\b|\\s*,\\s*', text)\n    return [s.strip() for s in splits if s]\n\ndef get_sentiment(text):\n    result = sentiment_pipeline(text)\n    return result[0]['label']\n\ndef get_absa(review):\n    sentences = sent_tokenize(review)\n    label_sentiments = {}\n\n    for sentence in sentences:\n        segments = split_on_conjunctions(sentence)\n        for segment in segments:\n            sentiment = get_sentiment(segment)\n            if sentiment == 'POSITIVE':\n                sentiment_label = \"Positive\"\n            elif sentiment == 'NEGATIVE':\n                sentiment_label = \"Negative\"\n            else:\n                sentiment_label = \"Neutral\"\n\n            labels, probabilities = model.predict(segment, threshold=threshold, k=num_labels)\n\n            for i, label in enumerate(labels):\n                stripped_label = label.replace(\"__label__\", \"\")\n                if stripped_label not in label_sentiments or probabilities[i] > label_sentiments[stripped_label][1]:\n                    label_sentiments[stripped_label] = (sentiment_label, probabilities[i])\n\n    # Removing the probabilities and keeping only the sentiments for final output\n    final_sentiments = {label: sentiment[0] for label, sentiment in label_sentiments.items()}\n    return final_sentiments\n    \n\nreview = (\"Easiest and most convenient way for a student to stream music,\" \n\" but the app is plagued with issues, like pausing randomly on my Samsung\" \n\" S8 *edit* Issues still persist on Samsung S21. I'm paying far too much for the \"\n\"premium I wish it cost less but I kind of enjoy the listening to the podcasts \"\n\"available. Please help  \"\n\"the customer support team is lacking as they don't communicate nor reply and I absolutely hate how slow it is to connect my device on wifi and all the buzzing sounds are annoying\")\n\nget_absa(review)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T00:08:14.256466Z","iopub.execute_input":"2023-09-16T00:08:14.256833Z","iopub.status.idle":"2023-09-16T00:08:15.484771Z","shell.execute_reply.started":"2023-09-16T00:08:14.256804Z","shell.execute_reply":"2023-09-16T00:08:15.483874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datafile = '/kaggle/input/random-test/randomly shuffled_reviews_only.csv'\ntest_df = pd.read_csv(datafile, encoding=\"utf-8\")\ntest_df_100 = test_df.head(100)\ntqdm.pandas(desc=\"Calculating Similarities\")\ntest_df_100['absa'] = test_df_100['Review'].progress_apply(lambda text: get_absa(text))","metadata":{"execution":{"iopub.status.busy":"2023-09-16T00:08:15.486347Z","iopub.execute_input":"2023-09-16T00:08:15.487009Z","iopub.status.idle":"2023-09-16T00:08:27.308734Z","shell.execute_reply.started":"2023-09-16T00:08:15.486968Z","shell.execute_reply":"2023-09-16T00:08:27.307671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_100","metadata":{"execution":{"iopub.status.busy":"2023-09-16T00:08:27.311680Z","iopub.execute_input":"2023-09-16T00:08:27.312070Z","iopub.status.idle":"2023-09-16T00:08:27.357498Z","shell.execute_reply.started":"2023-09-16T00:08:27.312038Z","shell.execute_reply":"2023-09-16T00:08:27.356187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A great job on my sample review!","metadata":{}},{"cell_type":"code","source":"test_df_100.to_csv(\"lda_model_output.csv\", encoding='utf-8', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T00:11:16.884977Z","iopub.execute_input":"2023-09-16T00:11:16.885447Z","iopub.status.idle":"2023-09-16T00:11:16.897062Z","shell.execute_reply.started":"2023-09-16T00:11:16.885412Z","shell.execute_reply":"2023-09-16T00:11:16.895572Z"},"trusted":true},"execution_count":null,"outputs":[]}]}